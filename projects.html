---
layout: page
title: Projects
---

<head>
  <style>
  a { text-decoration : none; }
  a:hover { text-decoration : underline; }
  a, a:visited { color : #0050e7; }
  p { font-size : 16px; }
  h3 { font-size : 18px; margin : 8; padding : 0; }
  .logo { width: 100 px; margin-right : 20px; float : none; border : 10px;}
  .project { clear : left; padding-bottom : 0px; }
  .project p { height : 180px; padding-top : 0px;}
  .project strong { font-size : 17px; color : #0000A0; }
  .project strong a { font-size : 17px; color : #0000A0; }
  </style>
  </head>

<div class="project">
    <img src="../static/projs/DIVotrack.png" class="logo">
    <p> 
      <strong>
      DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes
      </strong>
      <br>
      Shengyu Hao, Peiyuan Liu, Yibing Zhan, Kaixun Jin, Zuozhu Liu, Mingli Song, Jenq-Neng Hwang and Gaoang Wang
      <br>
      <font color="#E89B00">
      <em>International Journal of Computer Vision (IJCV), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2302.07676">[Paper]</a>
      <a href="https://huggingface.co/datasets/syhao777/DIVOTrack">[Dataset]</a>
      <a href="https://github.com/shengyuhao/DIVOTrack">[Code]</a>
      <img alt="" src="https://img.shields.io/github/stars/shengyuhao/DIVOTrack?style=social">
      <br>
      <font color="grey" size="2">
        A new cross-view multi-object tracking dataset for DIVerse Open scenes with dense tracking pedestrians in realistic and non-experimental environments.
      </font>
    </p>
</div>

<div class="project">
  <img src="../static/projs/stablevideo.png" class="logo">
  <p> 
    <strong>
    StableVideo: Text-driven Consistency-aware Diffusion Video Editing
    </strong>
    <br>
    Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu
    <br>
    <font color="#E89B00">
    <em>International Conference on Computer Vision (ICCV), 2023</em>
    </font>
    <br>
    <a href="https://rese1f.github.io/StableVideo/">[Website]</a>
    <a href="https://arxiv.org/abs/2308.09592">[Paper]</a>
   	<a href="https://huggingface.co/spaces/Reself/StableVideo">[Demo]</a>
    <a href="https://github.com/rese1f/StableVideo">[Code]</a>
    <img alt="" src="https://img.shields.io/github/stars/rese1f/StableVideo?style=social">
    <br>
    <font color="grey" size="2">
    We tackle introduce temporal dependency to existing text-driven diffusion models, which allows them to generate consistent appearance for the new objects.
    </font>
  </p>
</div>

<div class="project">
  <img src="../static/projs/moviechat.png" class="logo">
  <p> 
    <strong>
    MovieChat: From Dense Token to Sparse Memory in Long Video Understanding
    </strong>
    <br>
    Enxin Song*, Wenhao Chai*♡, Guanhong Wang*,Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Tian Ye, Jenq-Neng Hwang, Gaoang Wang✉
    <br>
    <em>arXiv Preprint.</em>
    <br>
    <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
    <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
    <a href="https://github.com/rese1f/MovieChat">[Dataset]</a>
    <a href="https://github.com/rese1f/MovieChat">[Code]</a>
    <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
    <br>
    <font color="grey" size="2">
    MovieChat achieves state-of-the-art performace in long video understanding by introducing memory mechanism.
    </font>
  </p>
</div>
