---
layout: diy
title: Publications
---

<head>
  <style>
    a {
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    a,
    a:visited {
      color: #6e6f71;
    }

    p {
      font-size: 16px;
    }

    h3 {
      font-size: 18px;
      margin: 8;
      padding: 0;
    }

    h4 {
      font-size: 16px;
      margin: 6;
      padding: 0;
    }

    .container {
      width: 1000px;
    }

    .publogo {
      width: 100 px;
      margin-right: 20px;
      float: left;
      border: 10px;
    }

    .publication {
      clear: left;
      padding-bottom: 0px;
    }

    .publication p {
      height: 180px;
      padding-top: 0px;
    }

    .publication strong {
      font-size: 17px;
      color: #990036;
    }

    .publication strong a {
      font-size: 17px;
      color: #990036;
    }
  </style>
</head>

<div class="container">


  <h3>2025</h3>
  <!-- M^3-Verse -->
  <div class="publication">
    <img src="../static/pubs/m3verse25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        M^3-Verse: A “Spot the Difference” Challenge for Large Multimodal Models
      </strong>
      <br>
      Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia,
      Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Arxiv, 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2512.18735">[Paper]</a>
      <a href="https://github.com/Wal-K-aWay/M3-Verse_pipeline">[Code]</a>
      <a href="https://www.modelscope.cn/datasets/WalKaWay/M3-Verse">[Dataset]</a>
      <!-- <a href="">[Website]</a> -->
      <img alt="" src="https://img.shields.io/github/stars/Wal-K-aWay/M3-Verse_pipeline?style=social">
      <br>
      <font color="grey" size="2">
        M^3-Verse enables evaluating Large Multimodal Models' ability to reason about object state changes in paired
        indoor videos, and proposes the HCTR method to boost multi-state perception performance.
      </font>
    </p>
  </div>

  <!-- MovieChat+ -->
  <div class="publication">
    <img src="../static/pubs/Mov+25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        MovieChat+: Question-Aware Sparse Memory for Long Video Question Answering
      </strong>
      <br>
      Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2404.17176">[Paper]</a>
      <a href="https://github.com/rese1f/MovieChat">[Code]</a>
      <!-- <a href="">[Dataset]</a> -->
      <!-- <a href="">[Website]</a> -->
      <img alt="" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
      <br>
      <font color="grey" size="2">
        Song et al. (2024) propose MovieChat+ with question-aware sparse memory to handle long video QA, outperforming
        SOTA, releasing
        MovieChat-1K benchmark, and cutting VRAM cost.
      </font>
    </p>
  </div>

  <!-- IGFuse -->
  <div class="publication">
    <img src="../static/pubs/IGFuse25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion
      </strong>
      <br>
      Wenhao Hu, Zesheng Li, Haonan Zhou, Liu Liu, Xuexiang Wen, Zhizhong Su,
      Xi Li, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Association for the Advancement of Artificial Intelligence (AAAI), 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2508.13153">[Paper]</a>
      <a href="https://github.com/whhu7/IGFuse-code">[Code]</a>
      <!-- <a href="">[Dataset]</a> -->
      <a href="https://whhu7.github.io/IGFuse">[Website]</a>
      <img alt="" src="https://img.shields.io/github/stars/whhu7/IGFuse-code?style=social">
      <br>
      <font color="grey" size="2">
        IGFuse enables high fidelity rendering and object level scene manipulation without dense observations or complex
        pipelines.
      </font>
    </p>
  </div>



 

  <!-- AGPMC -->
  <div class="publication">
    <img src="../static/pubs/AGPMC25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        Adaptive Graph Pruning for Multi-Agent Communication
      </strong>
      <br>
      Boyi Lia, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Arxiv, 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2506.02951">[Paper]</a>
      <a href="https://github.com/Resurgamm/AGP">[Code]</a>
      <a href="https://huggingface.co/datasets/Resurgammm/AGP-Training">[Dataset]</a>
      <a href="https://resurgamm.github.io/AGP/">[Website]</a>
      <img alt="" src="https://img.shields.io/github/stars/Resurgamm/AGP?style=social">
      <br>
      <font color="grey" size="2">
        Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes
        agent quantity (hardpruning) and communication topology (soft-pruning).
      </font>
    </p>
  </div>

  <!-- Video-MMLU -->
  <div class="publication">
    <img src="../static/pubs/VMMLU25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark
      </strong>
      <br>
      Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Arxiv, 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2504.14693">[Paper]</a>
      <a href="https://github.com/Espere-1119-Song/Video-MMLU/tree/main">[Code]</a>
      <a href="https://huggingface.co/datasets/Enxin/Video-MMLU">[Dataset]</a>
      <a href="https://enxinsong.com/Video-MMLU-web/">[Website]</a>
      <img alt="" src="https://img.shields.io/github/stars/Espere-1119-Song/Video-MMLU?style=social">
      <br>
      <font color="grey" size="2">
        Video-MMLU is a benchmark for evaluating LMMs' multi-discipline lecture understanding, covering math, physics,
        chemistry, with captioning and QA tasks, revealing model limitations.
      </font>
    </p>
  </div>



  <!-- PAPPC -->
  <div class="publication">
    <img src="../static/pubs/PAPPC25.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field
      </strong>
      <br>
      Wenhao Hu, Wenhao Chai, Shengyu Hao, Xiaotong Cui, Xuexiang Wen, Jenq-Neng Hwang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Arxiv, 2025</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2502.16303">[Paper]</a>
      <!-- <a href="CodeLink">[Code]</a> -->
      <!-- <a href="">[Dataset]</a> -->
      <a href="https://whuechoscript.github.io/CCGS/">[Website]</a>
      <!-- <img alt="" src="ShieldLink"> -->
      <br>
      <font color="grey" size="2">
        A method designed to achieve both view Consistent 2D segmentation and a
        Compact 3D Gaussian Segmentation field.
      </font>
    </p>
  </div>
  


  <h3>2024</h3>
  <!-- CityCraft -->
  <div class="publication">
    <img src="../static/pubs/CityCraft24.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        CityCraft: A Real Crafter for 3D City Generation
      </strong>
      <br>
      Jie Deng, Wenhao Chai, Junsheng Huang, Zhonghan Zhao, Mingyan Gao, Qixuan Huang, Jianshu Guo, Shengyu Hao, Wenhao
      Hu, Jenq-Neng Hwang, Xi Li, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Arxiv, 2024</em>
      </font>
      <br>
      <a href="PaperLink">[Paper]</a>
      <a href="https://github.com/djFatNerd/CityCraft">[Code]</a>
      <!-- <a href="">[Dataset]</a> -->
      <!-- <a href="">[Website]</a> -->
      <img alt="" src="https://img.shields.io/github/stars/djFatNerd/CityCraft?style=social">
      <br>
      <font color="grey" size="2">
        CityCraft, an innovative framework designed to enhance both the
        diversity and quality of urban scene generation.
      </font>
    </p>
  </div>

  <!-- VMMR24 -->
  <div class="publication">
    <img src="../static/pubs/VMMR24.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        Vision meets mmWave Radar: 3D Object Perception Benchmark for
        Autonomous Driving
      </strong>
      <br>
      Yizhou Wang, Jen-Hao Cheng, Jui-Te Huang, Sheng-Yao Kuan,
      Qiqian Fu, Chiming Ni, Shengyu Hao, Gaoang Wang,
      Guanbin Xing, Hui Liu, Jenq-Neng Hwang
      <br>
      <font color="#E89B00">
        <em>IEEE Intelligent Vehicles Symposium (IV), 2024</em>
      </font>
      <br>
      <a href="PaperLink">[Paper]</a>
      <!-- <a href="CodeLink">[Code]</a> -->
      <a href="https://huggingface.co/datasets/uwipl/CRUW3D">[Dataset]</a>
      <!-- <a href="">[Website]</a> -->
      <!-- <img alt="" src="ShieldLink"> -->
      <br>
      <font color="grey" size="2">
        CRUW3D dataset, including 66K synchronized and wellcalibrated camera, radar, and LiDAR frames in various driving
        scenarios.
      </font>
    </p>
  </div>

 


  <!-- MovieChat -->
  <div class="publication">
    <img src="../static/pubs/Mov24.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        MovieChat: From Dense Token to Sparse Memory in Long Video Understanding
      </strong>
      <br>
      Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye,
      Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Computer Vision and Pattern Recognition (CVPR), 2024</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2307.16449">[Paper]</a>
      <a href="https://github.com/rese1f/MovieChat">[Code]</a>
      <a href="https://huggingface.co/datasets/Enxin/MovieChat-1K_train">[Dataset]</a>
      <a href="https://rese1f.github.io/MovieChat/">[Website]</a>
      <img alt="" src="https://img.shields.io/github/stars/rese1f/MovieChat?style=social">
      <br>
      <font color="grey" size="2">
        MovieChat achieves state-of-the-art performace in long video understanding by introducing memory mechanism.
      </font>
    </p>
  </div>


  <!-- UniAP -->
  <div class="publication">
    <img src="../static/pubs/Uni24.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        UniAP: Towards Universal Animal Perception in Vision via Few-Shot Learning
      </strong>
      <br>
      Meiqi Sun, Zhonghan Zhao, Wenhao Chai, Hanjun Luo, Shidong Cao, Yanting Zhang, Jenq-Neng Hwang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>Association for the Advancement of Artificial Intelligence (AAAI), 2024</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2308.09953">[Paper]</a>
      <a href="https://github.com/rese1f/UniAP">[Code]</a>
      <a href="https://rese1f.github.io/UniAP/">[Website]</a>
      <img alt="" src="https://img.shields.io/github/stars/rese1f/UniAP?style=social">
      <br>
      <font color="grey" size="2">
        UniAP, a novel Universal Animal Perception model that leverages few-shot learning to enable cross-species
        perception among various visual tasks.
      </font>
    </p>
  </div>

 

  <h3>2023</h3>

  <!-- DIVOTrack -->
  <div class="publication">
    <img src="../static/pubs/DIV23.png" class="publogo" width="200 px" height="160 px">
    <p>
      <strong>
        DIVOTrack: A Novel Dataset and Baseline Method for Cross-View Multi-Object Tracking in DIVerse Open Scenes
      </strong>
      <br>
      Shengyu Hao, Peiyuan Liu, Yibing Zhan, Kaixun Jin, Zuozhu Liu, Mingli Song, Jenq-Neng Hwang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>International Journal of Computer Vision (IJCV), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2302.07676">[Paper]</a>
      <a href="https://huggingface.co/datasets/syhao777/DIVOTrack">[Dataset]</a>
      <a href="https://github.com/shengyuhao/DIVOTrack">[Code]</a>
      <img alt="" src="https://img.shields.io/github/stars/shengyuhao/DIVOTrack?style=social">
      <br>
      <font color="grey" size="2">
        A new cross-view multi-object tracking dataset for DIVerse Open scenes with dense tracking pedestrians.
      </font>
    </p>
  </div>

  <!-- RFD23 -->
  <div class="publication">
    <img src="../static/pubs/RFD23.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models
      </strong>
      <br>
      Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>IEEE Transactions on Multimedia (TMM), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2302.06826">[Paper]</a>
      <a href="https://github.com/Rem105-210/DiffFashion">[Code]</a>
      <img alt="" src="https://img.shields.io/github/stars/Rem105-210/DiffFashion?style=social">
      <br>
      <font color="grey" size="2">
        We focus on a new fashion design task, where we aim to transfer a reference appearance image onto a clothing
        image while preserving the structure of the clothing image.
      </font>
    </p>
  </div>


  <!-- STC23 -->
  <div class="publication">
    <img src="../static/pubs/STC23.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        StableVideo: Text-driven Consistency-aware Diffusion Video Editing
      </strong>
      <br>
      Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu
      <br>
      <font color="#E89B00">
        <em>International Conference on Computer Vision (ICCV), 2023</em>
      </font>
      <br>
      <a href="https://rese1f.github.io/StableVideo/">[Website]</a>
      <a href="https://arxiv.org/abs/2308.09592">[Paper]</a>
      <a href="https://huggingface.co/spaces/Reself/StableVideo">[Demo]</a>
      <a href="https://github.com/rese1f/StableVideo">[Code]</a>
      <img alt="" src="https://img.shields.io/github/stars/rese1f/StableVideo?style=social">
      <br>
      <font color="grey" size="2">
        We tackle introduce temporal dependency to existing text-driven diffusion models, which allows them to generate
        consistent appearance for the new objects.
      </font>
    </p>
  </div>


  <!-- GAM23 -->
  <div class="publication">
    <img src="../static/pubs/GAM23.png" class="publogo" width="200 px" height="160 px">
    <p>
      <strong>
        Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation
      </strong>
      <br>
      Wenhao Chai, Zhongyu Jiang, Jenq-Neng Hwang, Gaoang Wang
      <br>
      <font color="#E89B00">
        <em>International Conference on Computer Vision (ICCV), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2303.16456">[Paper]</a>
      <a href="https://github.com/rese1f/PoseDA">[Code]</a>
      <img alt="NPM" src="https://img.shields.io/github/stars/rese1f/PoseDA?style=social">
      <br>
      <font color="grey" size="2">
        A simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation.
      </font>
    </p>
  </div>



  <!-- PMP23 -->
  <div class="publication">
    <img src="../static/pubs/PMP23.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Enhanced 3D Human Pose Estimation
      </strong>
      <br>
      Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo,
      Yifeng Geng, Xuansong Xie
      <br>
      <font color="#E89B00">
        <em>ACM Multimedia (ACM MM), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/abs/2308.09678">[Paper]</a>
      <a href="https://github.com/hbing-l/PoSynDA">[Code]</a>
      <img alt="NPM" src="https://img.shields.io/github/stars/hbing-l/PoSynDA?style=social">
      <br>
      <font color="grey" size="2">
        PoSynDA offers a state-of-the-art domain adaptation solution for 3D pose estimation.
      </font>
    </p>
  </div>

  <div class="publication">
    <img src="../static/pubs/SGAT23.png" class="publogo" width="200 px" height="150 px">
    <p>
      <strong>
        SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic
        Segmentation
      </strong>
      <br>
      Xuewei Li, Tao Wu, Zhongang Qi, Gaoang Wang, Ying Shan, Xi Li
      <br>
      <font color="#E89B00">
        <em>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI), 2023</em>
      </font>
      <br>
      <a href="https://arxiv.org/pdf/2306.03403">[Paper]</a>
      <a href="https://github.com/TencentARC/SGAT4PASS">[Code]</a>
      <!-- <a href="">[Dataset]</a> -->
      <!-- <a href="">[Website]</a> -->
      <img alt="" src="https://img.shields.io/github/stars/TencentARC/SGAT4PASS?style=social">
      <br>
      <font color="grey" size="2">
        To be more robust to 3D disturbance, we propose our Spherical GeometryAware Transformer for PAnoramic Semantic
        Segmentation (SGAT4PASS), considering 3D spherical geometry knowledge.
      </font>
    </p>
  </div>



</div>